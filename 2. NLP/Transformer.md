[[Attention]] 

### 전이 학습 (Transer Learning)
- 특정 태스크를 학습한 모델을 다른 태스크 수행에 재사용하는 기법
- 전이 학습 방법은 **업스트림 태스크**, **다운스트림 태스크** 두 가지로 나뉜다.
### 업스트림 태스크 (Upstream task)
- 넓게 배우는 것. 넓게 배우기 위해 corpus를 학습한다. 대표적인 방법으로는 크게 2가지가 있다.
	1.  단어(문장) 맞히기 (Next Sentence Prediction, **NSP**)
		- 문장의 연관성을 학습하여 다음 단어가 무엇이지 맞히는 것
		
	1.  빈칸 맞히기 (Masked Language Model, **MLM**)
		- 단어에 빈칸을 둔 뒤 이를 맞히는 것
> Pre-training : 이러한 방법을 통해 업스트림 태스크를 수행하는 것
> BERT : 두 가지 방식 모두를 사용하여 업스트림(=pre-training)을 수행
> GPT : NSP 방법만을 통해 업스트림 하는 것
- 업스트림의 장점 : Self-Supervised Learning이 가능하다.
### 다운스트림 태스크 (Downstream task)
- 깊게 배우는 것. 업스트림을 통해 넓게 배운 다음 깊게 배우는 것. 다른 말로는 구체적으로 풀고자 하는 문제를 수행하는 것을 의미한다.
	1. 문장 분류(SC) : 문장에 대한 긍정, 부정. 중립에 대한 확률 값을 반환하는 Task
	2. 자연어 추론 (NLI) : 문장에 대한 참, 거짓, 중립 확률 값 반환하는 Task
	3. 개체명 인식 (NER) : 기관명, 인명, 지명 등 개체명 범주 확률값을 반환하는 Task
	4. 질의 응답 (QA) : 질문이 주어질 때 답변에 대한 확률 값을 반환하는 Task
	5. 문장 생성 : 문장을 입력 받고 어휘 전체에 대한 확률 값을 반환하는 Task
- 다운스트림 Task에는 파인 튜닝(FIne-tuning)과 프롬포트 튜닝(Prompt-tuning), 인컨텍스트 러닝(In-context-learning)이 있다.
	1. Fine-Tuning : 다운스트림 태스크 전체를 사용하는 것으로, 모델 전체를 업데이트 한다.
	 2. Prompt-Tuning : 다운스트림 태스크 데이터 전체를 사용해서 모델을 일부 업데이트 하는 방법이다.
	 3. In-Context-Learning : 다운스트림 태스크 데이터 일부만 사용. 업데이트하지 않고 다운스트림 태스크를 수행하는 방법이다.
		 - 제로샷 러닝 :  다운스트림 태스크 데이터를 전혀 사용하지 않고 모델이 바로 다운스트림 태스크를 수행하는 것.
		 - 원샷 러닝 : '' " 를 1건만 사용하는 것을 의미한다. 모델이 1건의 데이터가 어떻게 수행되는지 참고한 뒤 다운스트림 태스크를 수행
		 - 퓨샷 러닝 : '' '' 를 몇 건만 사용하는 것을 의미한다. 모델이 몇 건의 데이터가 " "
		 -


